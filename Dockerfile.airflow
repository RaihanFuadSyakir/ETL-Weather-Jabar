FROM apache/airflow:2.9.0


# Install Spark provider
# REQUIRED TO BE SAME VERSION SPARK AS SPARK MASTER CONTAINER
RUN pip install pyspark==3.5.6
RUN pip install --no-cache-dir "apache-airflow[spark]"
# RUN pip install --no-cache-dir apache-airflow-providers-apache-spark

USER root
RUN apt-get update && apt-get install -y --no-install-recommends openjdk-17-jre-headless \
    && apt-get autoremove -yqq --purge \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set JAVA_HOME so Spark knows where Java is
ENV JAVA_HOME="/usr/lib/jvm/java-17-openjdk-amd64"

# Switch back to airflow user
USER airflow

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

